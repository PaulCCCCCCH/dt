Now

Done. Use pad so that there is no need to calculate 'closest' during training
Done. Allow continuing from checkpoint
Done. Allow training separately
Done. Loss function update
-. Weighted embedding may not work very well.
-. Deal with OOV elegantly
-. Improve produced language
-. Finish manual control for pong
-. Try other games

-. Try probabilistic generation during eval
-. Pre-train language model separately

-. Evaluate generated language! Use N-gram log probability
-. Compare with previous ones
-. Produce graph from log
-. May use command following score (calculate command following accuracy)

Last resort:
Building the vocabulary from the instructions alone


Experiment

1. Language cannot be the only source of state repr. Try concatenating it with the original state repr.
2. Pre-train the language model and / or the A3C model alone
3. Lower the dimension of input for smaller games
4. Freeze & transfer language part
5. Try on run it on montezuma's revenge
6. Finish Manual control


Report

1. Background for multi-modal learning


